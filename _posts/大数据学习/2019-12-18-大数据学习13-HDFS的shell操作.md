---
layout: post
title: 大数据学习13--HDFS的shell操作
tags: [大数据]
categories: 大数据
typora-root-url: ..\..
---

下面进行对HDFS进行一些命令操作的练习。其中主要有`bin/hadoop fs 具体命令` 或者 `bin/hdfs dfs 具体命令`，而`dfs`又是`fs`的实现类。

<!-- TOC -->

- [1. 启动hadoop集群](#1-启动hadoop集群)
- [2. -help：输出这个命令参数](#2--help输出这个命令参数)
- [3. -ls：显示目录信息](#3--ls显示目录信息)
- [4. -mkdir：在hdfs上创建目录](#4--mkdir在hdfs上创建目录)
- [5. -moveFromLocal：从本地剪切粘贴到hdfs](#5--movefromlocal从本地剪切粘贴到hdfs)
- [6. -appendToFile：追加一个文件到已经存在的文件末尾](#6--appendtofile追加一个文件到已经存在的文件末尾)
- [7. -cat：显示文件内容](#7--cat显示文件内容)
- [8. -tail：显示一个文件的末尾](#8--tail显示一个文件的末尾)
- [9. -chgrp/-chmod/-chown：在linux中用法是一样的，修改文件所属权限](#9--chgrp-chmod-chown在linux中用法是一样的修改文件所属权限)
- [10. -copyFromLocal：从本地拷贝到hdfs](#10--copyfromlocal从本地拷贝到hdfs)
- [11. -copyToLocal：从hdfs拷贝到本地](#11--copytolocal从hdfs拷贝到本地)
- [12. -cp：从hdfs一个路径拷贝到hdfs另一个路径](#12--cp从hdfs一个路径拷贝到hdfs另一个路径)
- [13. -get：等同于copyToLocal，从hdfs下载到本地](#13--get等同于copytolocal从hdfs下载到本地)
- [14. -put：等同于copyFromLocal，本地上传到hdfs](#14--put等同于copyfromlocal本地上传到hdfs)
- [15. -rm：删除文件或者文件夹](#15--rm删除文件或者文件夹)
- [16. -rmdir：删除空目录](#16--rmdir删除空目录)
- [17. -du：统计文件夹的大小信息](#17--du统计文件夹的大小信息)
- [18. -setrep：设置hdfs中文件的副本数量](#18--setrep设置hdfs中文件的副本数量)

<!-- /TOC -->

# 1. 启动hadoop集群

启动Hadoop集群，具体的Hadoop相关配置请参考[hadoop完全分布式配置](/2019-12-10-大数据学习9--hadoop完全分布式配置之集群单点启动.md) 和 [群起集群](/2019-12-11-大数据学习10--ssh免密登录和群起集群/) 这两篇博客。

在hadoop102启动DHFS，在hadoop103启动YARN,输入如下命令，启动Hadoop集群：

```
[root@hadoop102 hadoop-2.10.0]# start-dfs.sh 
[root@hadoop103 hadoop-2.10.0]# start-yarn.sh 
```

# 2. -help：输出这个命令参数

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -help rm
-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :
  Delete all files that match the specified file pattern. Equivalent to the Unix
  command "rm <src>"
                                                                                 
  -f          If the file does not exist, do not display a diagnostic message or 
              modify the exit status to reflect an error.                        
  -[rR]       Recursively deletes directories.                                   
  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  
  -safely     option requires safety confirmation, if enabled, requires          
              confirmation before deleting large directory with more than        
              <hadoop.shell.delete.limit.num.files> files. Delay is expected when
              walking over large directory recursively to count the number of    
              files to be deleted before the confirmation.  
            

```

# 3. -ls：显示目录信息

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -ls /
Found 5 items
drwxr-xr-x   - root supergroup          0 2019-12-13 08:36 /ooooo
drwxr-xr-x   - root supergroup          0 2019-12-12 16:05 /output
drwx------   - root supergroup          0 2019-12-12 16:04 /tmp
drwxr-xr-x   - root supergroup          0 2019-12-12 16:02 /user
drwxr-xr-x   - root supergroup          0 2019-12-12 15:20 /wcinput
```

# 4. -mkdir：在hdfs上创建目录

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -mkdir -p /zzh/zohar
[root@hadoop102 hadoop-2.10.0]# hadoop fs -ls /
Found 6 items
drwxr-xr-x   - root supergroup          0 2019-12-13 08:36 /ooooo
drwxr-xr-x   - root supergroup          0 2019-12-12 16:05 /output
drwx------   - root supergroup          0 2019-12-12 16:04 /tmp
drwxr-xr-x   - root supergroup          0 2019-12-12 16:02 /user
drwxr-xr-x   - root supergroup          0 2019-12-12 15:20 /wcinput
drwxr-xr-x   - root supergroup          0 2019-12-19 14:10 /zzh
```
> -p：如果创建的文件已经存在也不失败。

# 5. -moveFromLocal：从本地剪切粘贴到hdfs

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -moveFromLocal shelltest.txt  /zzh/zohar
```

# 6. -appendToFile：追加一个文件到已经存在的文件末尾

```
[root@hadoop102 hadoop-2.10.0]# touch appendText.txt
[root@hadoop102 hadoop-2.10.0]# vim appendText.txt 
[root@hadoop102 hadoop-2.10.0]# hadoop fs -appendToFile appendText.txt /zzh/zohar/shelltext.txt
```

# 7. -cat：显示文件内容

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -cat /zzh/zohar/shelltext.txt
This is a append test!
```

# 8. -tail：显示一个文件的末尾

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -tail /zzh/zohar/shelltext.txt
This is a append test!
```

# 9. -chgrp/-chmod/-chown：在linux中用法是一样的，修改文件所属权限

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -chmod 666 /zzh/zohar/shelltext.txt
[root@hadoop102 hadoop-2.10.0]# hadoop fs -chown zohar:zohar /zzh/zohar/shelltext.txt
```

# 10. -copyFromLocal：从本地拷贝到hdfs

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -copyFromLocal README.txt /
```

# 11. -copyToLocal：从hdfs拷贝到本地

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -copyToLocal /zzh/zohar/shelltext.txt ./
```

# 12. -cp：从hdfs一个路径拷贝到hdfs另一个路径

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -cp /zzh/zohar/shelltext.txt /haha.txt
[root@hadoop102 hadoop-2.10.0]# hadoop fs -cat /haha.txt
This is a append test!
```

# 13. -get：等同于copyToLocal，从hdfs下载到本地

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -get /zzh/zohar/shelltext.txt ./
```

# 14. -put：等同于copyFromLocal，本地上传到hdfs


# 15. -rm：删除文件或者文件夹

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -rm /zzh/zohar/shelltext.txt
Deleted /zzh/zohar/shelltext.txt
```

# 16. -rmdir：删除空目录

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -mkdir /text
[root@hadoop102 hadoop-2.10.0]# hadoop fs -rmdir /text
```

# 17. -du：统计文件夹的大小信息

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -du -s -h /
463.4 K  /

[root@hadoop102 hadoop-2.10.0]# hadoop fs -du  -h /
1.3 K    /README.txt
23       /haha.txt
55       /ooooo
55       /output
461.8 K  /tmp
0        /user
59       /wcinput
0        /zzh

```

# 18. -setrep：设置hdfs中文件的副本数量

```
[root@hadoop102 hadoop-2.10.0]# hadoop fs -setrep 10 /zzh/zohar/shelltext.txt
```
> 这里设置副本数只是记录在namenode中，但是是否真的会有这么多副本还是得看datanode数量。当前只有3台设备，随意最多只有3个副本，只有当节点数目增加到10台时，副本数才能达到10.

